{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq5ox1isOjQf"
   },
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg57fvipOuU5"
   },
   "source": [
    "### part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kFWr6eI04a4H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YP4KFTzjVI-2",
    "outputId": "a7211734-7ba5-4899-baee-787e56d32b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point: [2.03703598e-10 2.03703598e-10]\n",
      "Optimized function value: 8.299031137761999e-20\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    # Define the function you want to optimize\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_f(x):\n",
    "    # Compute the gradient of the function at point x\n",
    "    return np.array([2 * x[0], 2 * x[1]])\n",
    "\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_f(x)\n",
    "        x = x - learning_rate * gradient\n",
    "\n",
    "    return x\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([1.0, 1.0])\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "# Run gradient descent\n",
    "optimized_point = gradient_descent(initial_point, learning_rate, num_iterations)\n",
    "\n",
    "print(\"Optimized point:\", optimized_point)\n",
    "print(\"Optimized function value:\", f(optimized_point))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTdIDRRyJpGg"
   },
   "source": [
    "### part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFijNPCjJl6f",
    "outputId": "daa491a7-b889-430e-b5a0-5b0b12cf431d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point: [4.70882387e-22 4.70882387e-22]\n",
      "Optimized function value: 4.434604456688442e-43\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "def f(x):\n",
    "    # Define the function you want to optimize\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_momentum_f(x, learning_momentum, v):\n",
    "    # Compute the gradient with the learning momentum\n",
    "    return np.array([2 * x[0] - learning_momentum * 2 * v[0], 2 * x[1] - learning_momentum * 2 * v[1]])\n",
    "\n",
    "def gradient_descent_momentum(initial_x, learning_rate, learning_momentum, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_momentum_f(x, learning_momentum, v)\n",
    "        v = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x = x - v\n",
    "    return x\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([1.0, 1.0])\n",
    "learning_rate = 0.1\n",
    "learning_momentum = 0.5\n",
    "num_iterations = 100\n",
    "\n",
    "# Run gradient descent\n",
    "optimized_point = gradient_descent_momentum(initial_point, learning_rate, learning_momentum, num_iterations)\n",
    "\n",
    "print(\"Optimized point:\", optimized_point)\n",
    "print(\"Optimized function value:\", f(optimized_point))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Npz9BEcaM1rO"
   },
   "source": [
    "### part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XV5kEQfCM3U4",
    "outputId": "0e15dc09-e1bb-4af3-fda3-476bf1cb1e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point (standard): [2.03703598e-10 2.03703598e-10]\n",
      "Optimized function value (standard): 8.299031137761999e-20\n",
      "Optimized point (momentum): [4.70882387e-22 4.70882387e-22]\n",
      "Optimized function value (momentum): 4.434604456688442e-43\n",
      "Optimized point (momentum): [-2.2258194e-06 -2.2258194e-06]\n",
      "Optimized function value (momentum): 9.908543958645736e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x213154ef220>,\n",
       " <matplotlib.lines.Line2D at 0x213154ef310>,\n",
       " <matplotlib.lines.Line2D at 0x213154ef3d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FUNCTION 1\n",
    "import pdb\n",
    "\n",
    "def f1(fx):\n",
    "    # Define the first function you want to optimize\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_f1(fx):\n",
    "    # Compute the gradient of the function at point x\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    return np.array([2 * x, 2 * y])\n",
    "\n",
    "def gradient_momentum_f1(fx, learning_momentum, v):\n",
    "    # Compute the gradient with the learning momentum\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    return np.array([2 * x - learning_momentum * 2 * v[0], 2 * y - learning_momentum * 2 * v[1]])\n",
    "\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    \n",
    "    y_actual = np.array([0]*num_iterations)\n",
    "    y_pred = []\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_f1(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        y_pred.append(x[0]) \n",
    "        steps.append(i)\n",
    "        \n",
    "    steps = np.array(steps)\n",
    "    error = np.array([y_pred[i] - y_actual[i] for i in range(len(y_pred))])\n",
    "    y_pred = np.array(y_pred)\n",
    "    # plt.plot(steps, error)\n",
    "    return [x, steps, error]\n",
    "\n",
    "def gradient_descent_momentum(initial_x, learning_rate, learning_momentum, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    y_actual = np.array([0]*num_iterations)\n",
    "    y_pred = []\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_momentum_f1(x, learning_momentum, v)\n",
    "        v = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x = x - v\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        y_pred.append(x[0]) \n",
    "        steps.append(i)\n",
    "        \n",
    "    steps = np.array(steps)\n",
    "    error = np.array([y_pred[i] - y_actual[i] for i in range(len(y_pred))])\n",
    "    y_pred = np.array(y_pred)\n",
    "    return [x, steps, error]\n",
    "\n",
    "def gradient_descent_varying(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "    \n",
    "    y_actual = np.array([0]*num_iterations)\n",
    "    y_pred = []\n",
    "    steps = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        learning_momentum = (i+1)/(i+2)\n",
    "        gradient = gradient_momentum_f1(x, learning_momentum, v)\n",
    "        v = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x = x - v\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # pdb.set_trace()\n",
    "        \n",
    "        y_pred.append(x[0]) \n",
    "        steps.append(i)\n",
    "        \n",
    "    steps = np.array(steps)\n",
    "    error = np.array([y_pred[i] - y_actual[i] for i in range(len(y_pred))])\n",
    "    y_pred = np.array(y_pred)\n",
    "    return [x, steps, error]\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([1.0, 1.0])\n",
    "learning_rate = 0.1\n",
    "learning_momentum = 0.5\n",
    "num_iterations = 100\n",
    "\n",
    "# Run gradient descent\n",
    "optimized_point = gradient_descent(initial_point, learning_rate, num_iterations)[0]\n",
    "steps, error = gradient_descent(initial_point, learning_rate, num_iterations)[1:3]\n",
    "\n",
    "print(\"Optimized point (standard):\", optimized_point)\n",
    "print(\"Optimized function value (standard):\", f1(optimized_point))\n",
    "\n",
    "optimized_point = gradient_descent_momentum(initial_point, learning_rate, learning_momentum, num_iterations)[0]\n",
    "steps_momentum, error_momentum = gradient_descent(initial_point, learning_rate, num_iterations)[1:3]\n",
    "print(\"Optimized point (momentum):\", optimized_point)\n",
    "print(\"Optimized function value (momentum):\", f1(optimized_point))\n",
    "\n",
    "optimized_point = gradient_descent_varying(initial_point, learning_rate, num_iterations)[0]\n",
    "steps_varying, error_varying = gradient_descent(initial_point, learning_rate, num_iterations)[1:3]\n",
    "print(\"Optimized point (momentum):\", optimized_point)\n",
    "print(\"Optimized function value (momentum):\", f1(optimized_point))\n",
    "\n",
    "plt.plot(steps, error, steps, error_momentum, steps, error_varying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7gXMJvclLhD",
    "outputId": "d4f5be5b-41d7-45e1-9f2e-55f2738da9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point (standard): [1.47257713 2.16768049]\n",
      "Optimized function value (standard): 0.22339360489202056\n",
      "Optimized point (momentum): [1.69092511 2.8612798 ]\n",
      "Optimized function value (momentum): 0.4777986064176193\n",
      "Optimized point (momentum): [0.08546917 0.00728141]\n",
      "Optimized function value (momentum): 0.8363666990312171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x213167d7100>,\n",
       " <matplotlib.lines.Line2D at 0x213167d71f0>,\n",
       " <matplotlib.lines.Line2D at 0x213167d72b0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FUNCTION 2\n",
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "def f2(fx):\n",
    "    # Define the second function to optimize\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    f = (1-x)**2 + (100 * (y - x**2)**2)\n",
    "    return f\n",
    "\n",
    "def gradient_f2(fx):\n",
    "    # Compute the gradient of the function at point x\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    dfdx = (400 * x**3) + x - (400 * x * y) - 2\n",
    "    dfdy = (200 * y) - (200 * x**2)\n",
    "    return np.array([dfdx, dfdy])\n",
    "\n",
    "def gradient_momentum_f2(fx, learning_momentum, v):\n",
    "    # Compute the gradient of with learning momentum\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    dfdx = (400 * x**3) + x - (400 * x * y) - 2\n",
    "    dfdy = (200 * y) - (200 * x**2)\n",
    "\n",
    "    u = v[0]\n",
    "    w = v[1]\n",
    "    dvdu = (400 * u**3) + u - (400 * u * w) - 2\n",
    "    dvdw = (200 * w) - (200 * u**2)\n",
    "    return np.array([dfdx - learning_momentum * dvdu, dfdy - learning_momentum * dvdw])\n",
    "\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "\n",
    "    y_actual = np.array([0]*num_iterations)\n",
    "    y_pred = []\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_f2(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        # print(\"base\")\n",
    "        # pdb.set_trace()\n",
    "    \n",
    "        y_pred.append(x[0]) \n",
    "        steps.append(i)\n",
    "        \n",
    "    steps = np.array(steps)\n",
    "    error = np.array([y_pred[i] - y_actual[i] for i in range(len(y_pred))])\n",
    "    y_pred = np.array(y_pred)\n",
    "    return [x, steps, error]\n",
    "    \n",
    "\n",
    "def gradient_descent_momentum(initial_x, learning_rate, learning_momentum, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    y_actual = np.array([0]*num_iterations)\n",
    "    y_pred = []\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_momentum_f2(x, learning_momentum, v)\n",
    "        v = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x = x - v\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # print(\"momentum\")\n",
    "        #pdb.set_trace()\n",
    "    \n",
    "        y_pred.append(x[0]) \n",
    "        steps.append(i)\n",
    "        \n",
    "    steps = np.array(steps)\n",
    "    error = np.array([y_pred[i] - y_actual[i] for i in range(len(y_pred))])\n",
    "    y_pred = np.array(y_pred)\n",
    "    return [x, steps, error]\n",
    "\n",
    "def gradient_descent_varying(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    y_actual = np.array([0]*num_iterations)\n",
    "    y_pred = []\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        learning_momentum = (i+1)/(i+2)\n",
    "        gradient = gradient_momentum_f2(x, learning_momentum, v)\n",
    "        v = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x = x - v\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # print(\"varying\")\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        y_pred.append(x[0]) \n",
    "        steps.append(i)\n",
    "    \n",
    "    steps = np.array(steps)\n",
    "    error = np.array([y_pred[i] - y_actual[i] for i in range(len(y_pred))])\n",
    "    y_pred = np.array(y_pred)\n",
    "    return [x, steps, error]\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([2.0, 2.0])\n",
    "learning_rate = 0.0001\n",
    "learning_momentum = 0.9\n",
    "num_iterations = 1000\n",
    "\n",
    "# Run gradient descent\n",
    "optimized_point = gradient_descent(initial_point, learning_rate, num_iterations)[0]\n",
    "steps, error = gradient_descent(initial_point, learning_rate, num_iterations)[1:3]\n",
    "print(\"Optimized point (standard):\", optimized_point)\n",
    "print(\"Optimized function value (standard):\", f2(optimized_point))\n",
    "\n",
    "optimized_point = gradient_descent_momentum(initial_point, learning_rate, learning_momentum, num_iterations)[0]\n",
    "steps_momentum, error_momentum = gradient_descent(initial_point, learning_rate, num_iterations)[1:3]\n",
    "print(\"Optimized point (momentum):\", optimized_point)\n",
    "print(\"Optimized function value (momentum):\", f2(optimized_point))\n",
    "\n",
    "optimized_point = gradient_descent_varying(initial_point, learning_rate, num_iterations)[0]\n",
    "steps_varying, error_varying = gradient_descent(initial_point, learning_rate, num_iterations)[1:3]\n",
    "print(\"Optimized point (momentum):\", optimized_point)\n",
    "print(\"Optimized function value (momentum):\", f2(optimized_point))\n",
    "\n",
    "plt.plot(steps, error, steps, error_momentum, steps, error_varying)\n",
    "#plt.plot(steps, error)\n",
    "#plt.plot(steps, error_momentum)\n",
    "#plt.plot(steps, error_varying)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6gWOaDD484i"
   },
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rIu7jO0yNxD"
   },
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ShEL7MVvJw4",
    "outputId": "9ff3269d-98e4-4c20-bdba-9aef287bf97d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### FETCH DATA, ASSIGN VARS\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n",
    "\n",
    "X,y = mnist[\"data\"], mnist[\"target\"]\n",
    "X = np.array(X)\n",
    "some_digit = X[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1p_oIuyDvqp"
   },
   "source": [
    "Our 3 classifiers:\n",
    "1. OvA\n",
    "2. Random Forest\n",
    "3. KNeighbors (probably will work best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMeXfs--EaVq"
   },
   "source": [
    "## One vs. All (OvA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oODiwTYjEmS1",
    "outputId": "8548a52d-5d4e-4bcc-e800-0b4333620834"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8780295041097735"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])\n",
    "\n",
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "\n",
    "y_train_ova_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=5)\n",
    "f1_score(y_train, y_train_ova_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZCrR7b6T-TC"
   },
   "source": [
    "The One vs. All classifier was the slowest and least accurate of the 3 classifiers we chose. It took 16 minutes to produce an f1 score of 87%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tWE_A_FEb1D"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ut8t8qAJE8W5",
    "outputId": "bf3f88e4-bb59-461a-9cd7-05d95ae1e1d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661919580821857"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])\n",
    "\n",
    "forest_clf.predict_proba([some_digit])\n",
    "\n",
    "y_train_forest_pred = cross_val_predict(forest_clf, X_train, y_train, cv=5)\n",
    "f1_score(y_train, y_train_forest_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWERnAJAUFWG"
   },
   "source": [
    "The Random Forest classifier and the K Neighbors classifier scored very similarly, but because it's more difficult to tune the hyperparameters for this classifier, it is not the best one of the 3 we chose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMqh5EUyEdhy"
   },
   "source": [
    "## K Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4MS5NqJEfU9",
    "outputId": "fd1b0465-e2fe-4c5a-ecf8-79bcd897ea7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625164092416195"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)\n",
    "\n",
    "knn_clf.predict([some_digit])\n",
    "\n",
    "\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=5)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "\n",
    "### TEST\n",
    "\n",
    "y_test_large = (y_test >= 7)\n",
    "y_test_odd = (y_test % 2 == 1)\n",
    "y_multilabel_test = np.c_[y_test_large, y_test_odd]\n",
    "\n",
    "y_test_knn_pred = cross_val_predict(knn_clf, X_test, y_multilabel_test, cv=5)\n",
    "f1_score(y_multilabel_test, y_test_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiM93wiGRFTN"
   },
   "source": [
    "The K Nearest Neighbors classifier is the best of the 3 classifiers we chose. While it performed very similarly to the Random Forest classifier in this example, further tuning of hyperparameters would improve its performance even more."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
