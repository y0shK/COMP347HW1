{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq5ox1isOjQf"
   },
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg57fvipOuU5"
   },
   "source": [
    "### part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFWr6eI04a4H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YP4KFTzjVI-2",
    "outputId": "dde3641a-6164-4a67-ef8d-836a5243cd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point: [2.03703598e-10 2.03703598e-10]\n",
      "Optimized function value: 8.299031137761999e-20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def f(x):\n",
    "    # Define the function you want to optimize\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_f(x):\n",
    "    # Compute the gradient of the function at point x\n",
    "    return np.array([2 * x[0], 2 * x[1]])\n",
    "\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_f(x)\n",
    "        x = x - learning_rate * gradient\n",
    "\n",
    "    return x\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([1.0, 1.0])\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "# Run gradient descent\n",
    "optimized_point = gradient_descent(initial_point, learning_rate, num_iterations)\n",
    "\n",
    "print(\"Optimized point:\", optimized_point)\n",
    "print(\"Optimized function value:\", f(optimized_point))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTdIDRRyJpGg"
   },
   "source": [
    "### part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFijNPCjJl6f",
    "outputId": "9410ae2c-3c03-4842-a421-b22ca55e3dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point: [3.33503993e-12 3.33503993e-12]\n",
      "Optimized function value: 2.2244982618076215e-23\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "\n",
    "def f(x):\n",
    "    # Define the function you want to optimize\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_momentum_f(x, learning_momentum, v):\n",
    "    # Compute the gradient with the learning momentum\n",
    "    a = x[0] - learning_momentum * v[0]\n",
    "    b = x[1] - learning_momentum * v[1]\n",
    "    return np.array([2 * a, 2 * b])\n",
    "\n",
    "def gradient_descent_momentum(initial_x, learning_rate, learning_momentum, num_iterations):\n",
    "    # anh's notes:\n",
    "    # 'evaluate at the looking ahead point, not summing gradient at two different points'\n",
    "    # computation of error term is wrong?\n",
    "    # what is the error term\n",
    "    # gradient for f_2 is not (mathematically) correct\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_momentum_f(x, learning_momentum, v)\n",
    "        v_next = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x_next = x - v\n",
    "        v = v_next\n",
    "        x = x_next\n",
    "    return x\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([1.0, 1.0])\n",
    "learning_rate = 0.1\n",
    "learning_momentum = 0.5\n",
    "num_iterations = 100\n",
    "\n",
    "# Run gradient descent\n",
    "optimized_point = gradient_descent_momentum(initial_point, learning_rate, learning_momentum, num_iterations)\n",
    "\n",
    "print(\"Optimized point:\", optimized_point)\n",
    "print(\"Optimized function value:\", f(optimized_point))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Npz9BEcaM1rO"
   },
   "source": [
    "### part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "XV5kEQfCM3U4",
    "outputId": "72a9de6e-51e4-4ce3-d4e4-22b3fddf64cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point (standard): [2.03703598e-10 2.03703598e-10]\n",
      "Optimized function value (standard): 8.299031137761999e-20\n",
      "Optimized point (momentum): [3.33503993e-12 3.33503993e-12]\n",
      "Optimized function value (momentum): 2.2244982618076215e-23\n",
      "Optimized point (varying): [-0.26963264 -0.26963264]\n",
      "Optimized function value (varying): 0.1454035212810389\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FUNCTION 1\n",
    "import pdb\n",
    "\n",
    "def f1(fx):\n",
    "    # Define the first function you want to optimize\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_f1(fx):\n",
    "    # Compute the gradient of the function at point x\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    return np.array([2 * x, 2 * y])\n",
    "\n",
    "def gradient_momentum_f1(fx, learning_momentum, v):\n",
    "    # Compute the gradient with the learning momentum\n",
    "    a = fx[0] - learning_momentum * v[0]\n",
    "    b = fx[1] - learning_momentum * v[1]\n",
    "    return np.array([2 * a, 2 * b])\n",
    "\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    # desc_steps = ([[initial_x]])\n",
    "\n",
    "    # i am like maybe almost there to figuring out what i'm trying to do here\n",
    "    # -max\n",
    "    # if you can figure out what i have going on better than me\n",
    "    # be my guest\n",
    "\n",
    "    # try y_pred - y_actual to get the error\n",
    "    y_actual = np.array([0,0]) # for f1\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_f1(x)\n",
    "        x = x - learning_rate * gradient\n",
    "        # np.append(desc_steps,x)\n",
    "        \n",
    "        # this is what was on the whiteboard earlier\n",
    "        y_pred = np.array([x[0], x[0]])\n",
    "        error = np.linalg.norm(y_actual - y_pred)\n",
    "        \n",
    "        # where are error and steps being concatenated to create ordered pairs for graph? \n",
    "        # steps is i (x axis) and error is y-axis\n",
    "        # we should do that here\n",
    "        \n",
    "\n",
    "    return x\n",
    "\n",
    "def gradient_descent_momentum(initial_x, learning_rate, learning_momentum, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        gradient = gradient_momentum_f1(x, learning_momentum, v)\n",
    "        v_next = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x_next = x - v\n",
    "        v = v_next\n",
    "        x = x_next\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # pdb.set_trace()\n",
    "    return x\n",
    "\n",
    "def gradient_descent_varying(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        learning_momentum = (i+1)/(i+2)\n",
    "        gradient = gradient_momentum_f1(x, learning_momentum, v)\n",
    "        v_next = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x_next = x - v\n",
    "        v = v_next\n",
    "        x = x_next\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # pdb.set_trace()\n",
    "    return x\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([1.0, 1.0]) # np.random.randn(2,1) - will generate random array shaped 2,1\n",
    "learning_rate = 0.1\n",
    "learning_momentum = 0.5\n",
    "num_iterations = 100\n",
    "\n",
    "# Run gradient descent\n",
    "standard_optimized_point = gradient_descent(initial_point, learning_rate, num_iterations)\n",
    "print(\"Optimized point (standard):\", standard_optimized_point)\n",
    "print(\"Optimized function value (standard):\", f1(standard_optimized_point))\n",
    "\n",
    "momentum_optimized_point = gradient_descent_momentum(initial_point, learning_rate, learning_momentum, num_iterations)\n",
    "print(\"Optimized point (momentum):\", momentum_optimized_point)\n",
    "print(\"Optimized function value (momentum):\", f1(momentum_optimized_point))\n",
    "\n",
    "varying_optimized_point = gradient_descent_varying(initial_point, learning_rate, num_iterations)\n",
    "print(\"Optimized point (varying):\", varying_optimized_point)\n",
    "print(\"Optimized function value (varying):\", f1(varying_optimized_point))\n",
    "\n",
    "# plotting\n",
    "figure, axes = plt.subplots()\n",
    "# plt.plot(standard_op)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "V7gXMJvclLhD",
    "outputId": "a2af2e90-5069-4207-b573-f467cf205324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized point (standard): [1.37399294 1.88905855]\n",
      "Optimized function value (standard): 0.14001518651333114\n",
      "Optimized point (momentum): [0.98113329 0.96254618]\n",
      "Optimized function value (momentum): 0.00035653577023604915\n",
      "Optimized point (varying): [0.97153622 1.03248656]\n",
      "Optimized function value (varying): 0.7858757263831859\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### FUNCTION 2\n",
    "import pdb\n",
    "\n",
    "def f2(fx):\n",
    "    # Define the second function to optimize\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    f = (1-x)**2 + (100 * (y - x**2)**2)\n",
    "    return f\n",
    "\n",
    "def gradient_f2(fx):\n",
    "    # Compute the gradient of the function at point x\n",
    "    x = fx[0]\n",
    "    y = fx[1]\n",
    "    dfdx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dfdy = (200 * y) - (200 * x**2)\n",
    "    return np.array([dfdx, dfdy])\n",
    "\n",
    "def gradient_momentum_f2(fx, learning_momentum, v):\n",
    "    # Compute the gradient of with learning momentum\n",
    "    x = fx[0] - learning_momentum * v[0]\n",
    "    y = fx[1] - learning_momentum * v[1]\n",
    "    dfdx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dfdy = (200 * y) - (200 * x**2)\n",
    "    return np.array([dfdx, dfdy])\n",
    "\n",
    "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # print(\"normal, iteration:\", i)\n",
    "        gradient = gradient_f2(x)\n",
    "        x = x - learning_rate * gradient\n",
    "    return x\n",
    "\n",
    "def gradient_descent_momentum(initial_x, learning_rate, learning_momentum, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # print(\"momentum, iteration:\", i)\n",
    "        gradient = gradient_momentum_f2(x, learning_momentum, v)\n",
    "        v_next = np.add((learning_momentum * v), (learning_rate * gradient))\n",
    "        x_next = np.add(x, -1 * v)\n",
    "        # pdb.set_trace()\n",
    "        v = v_next\n",
    "        x = x_next\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # pdb.set_trace()\n",
    "    return x\n",
    "\n",
    "def gradient_descent_varying(initial_x, learning_rate, num_iterations):\n",
    "    x = initial_x\n",
    "    v = initial_x\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # print(\"varying, iteration:\", i)\n",
    "        learning_momentum = (i+1)/(i+2)\n",
    "        gradient = gradient_momentum_f2(x, learning_momentum, v)\n",
    "        v_next = (learning_momentum * v) + (learning_rate * gradient)\n",
    "        x_next = x - v\n",
    "        v = v_next\n",
    "        x = x_next\n",
    "        # print(\"iteration \" + str(i))\n",
    "        # pdb.set_trace()\n",
    "    return x\n",
    "\n",
    "# Set the initial point and learning rate\n",
    "initial_point = np.array([2.0, 2.0])\n",
    "learning_rate = 0.0001\n",
    "learning_momentum = 0.9\n",
    "num_iterations = 10000\n",
    "\n",
    "# Run gradient descent\n",
    "standard_optimized_point = gradient_descent(initial_point, learning_rate, num_iterations)\n",
    "print(\"Optimized point (standard):\", standard_optimized_point)\n",
    "print(\"Optimized function value (standard):\", f2(standard_optimized_point))\n",
    "\n",
    "momentum_optimized_point = gradient_descent_momentum(initial_point, learning_rate, learning_momentum, num_iterations)\n",
    "print(\"Optimized point (momentum):\", momentum_optimized_point)\n",
    "print(\"Optimized function value (momentum):\", f2(momentum_optimized_point))\n",
    "\n",
    "varying_optimized_point = gradient_descent_varying(initial_point, learning_rate, num_iterations)\n",
    "print(\"Optimized point (varying):\", varying_optimized_point)\n",
    "print(\"Optimized function value (varying):\", f2(varying_optimized_point))\n",
    "\n",
    "# plotting\n",
    "# figure, ax = plt.subplots()\n",
    "# plt.grid(True)\n",
    "# ax.plot(standard_optimized_point)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6gWOaDD484i"
   },
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rIu7jO0yNxD"
   },
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ShEL7MVvJw4",
    "outputId": "a467e692-dafd-44b7-b998-215deb540160"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "### FETCH DATA, ASSIGN VARS\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n",
    "\n",
    "X,y = mnist[\"data\"], mnist[\"target\"]\n",
    "X = np.array(X)\n",
    "some_digit = X[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1p_oIuyDvqp"
   },
   "source": [
    "Our 3 classifiers:\n",
    "1. OvA\n",
    "2. Random Forest\n",
    "3. KNeighbors (probably will work best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMeXfs--EaVq"
   },
   "source": [
    "## One vs. All (OvA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "oODiwTYjEmS1",
    "outputId": "69fe4f4e-f286-4415-bfee-42db3bbe6f74"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-12ccd8d722dd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msgd_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msgd_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msgd_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msome_digit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msome_digit_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msome_digit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_more_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self._fit(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         self._partial_fit(\n\u001b[0m\u001b[1;32m    684\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;31m# delegate to concrete training procedure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             self._fit_multiclass(\n\u001b[0m\u001b[1;32m    618\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_multiclass\u001b[0;34m(self, X, y, alpha, C, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_INT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         result = Parallel(\n\u001b[0m\u001b[1;32m    761\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sharedmem\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit_binary\u001b[0;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     coef, intercept, average_coef, average_intercept, n_iter_ = _plain_sgd(\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])\n",
    "\n",
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "\n",
    "y_train_ova_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=5)\n",
    "f1_score(y_train, y_train_ova_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZCrR7b6T-TC"
   },
   "source": [
    "The One vs. All classifier was the slowest and least accurate of the 3 classifiers we chose. It took 16 minutes to produce an f1 score of 87%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tWE_A_FEb1D"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut8t8qAJE8W5"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.predict([some_digit])\n",
    "\n",
    "forest_clf.predict_proba([some_digit])\n",
    "\n",
    "y_train_forest_pred = cross_val_predict(forest_clf, X_train, y_train, cv=5)\n",
    "f1_score(y_train, y_train_forest_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWERnAJAUFWG"
   },
   "source": [
    "The Random Forest classifier and the K Neighbors classifier scored very similarly, but because it's more difficult to tune the hyperparameters for this classifier, it is not the best one of the 3 we chose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMqh5EUyEdhy"
   },
   "source": [
    "## K Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4MS5NqJEfU9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)\n",
    "\n",
    "knn_clf.predict([some_digit])\n",
    "\n",
    "\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=5)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "\n",
    "### TEST\n",
    "\n",
    "y_test_large = (y_test >= 7)\n",
    "y_test_odd = (y_test % 2 == 1)\n",
    "y_multilabel_test = np.c_[y_test_large, y_test_odd]\n",
    "\n",
    "y_test_knn_pred = cross_val_predict(knn_clf, X_test, y_multilabel_test, cv=5)\n",
    "f1_score(y_multilabel_test, y_test_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiM93wiGRFTN"
   },
   "source": [
    "The K Nearest Neighbors classifier is the best of the 3 classifiers we chose. While it performed very similarly to the Random Forest classifier in this example, further tuning of hyperparameters would improve its performance even more."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
